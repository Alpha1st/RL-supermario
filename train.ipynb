{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym.spaces import Box\n",
    "from gym import Wrapper\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d339bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "def create_train_env(world, stage, action_type):\n",
    "    # 创建基础环境\n",
    "\n",
    "    if action_type == \"right\":\n",
    "        actions = RIGHT_ONLY\n",
    "    elif action_type == \"simple\":\n",
    "        actions = SIMPLE_MOVEMENT\n",
    "    else:\n",
    "       actions = COMPLEX_MOVEMENT\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-{}-{}-v3\".format(world, stage))\n",
    "    env = JoypadSpace(env, actions) \n",
    "\n",
    "    # 对环境自定义\n",
    "    env = CustomReward(env, world, stage, monitor=None)\n",
    "    env = CustomSkipFrame(env)\n",
    "    return env\n",
    "# 对原始环境进行修改，以获得更好的训练效果\n",
    "class CustomReward(Wrapper):\n",
    "    def __init__(self, env=None, world=None, stage=None, monitor=None):\n",
    "        super(CustomReward, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n",
    "        self.curr_score = 0\n",
    "        self.current_x = 40\n",
    "        self.world = world\n",
    "        self.stage = stage\n",
    "        if monitor:\n",
    "            self.monitor = monitor\n",
    "        else:\n",
    "            self.monitor = None\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        if self.monitor:\n",
    "            self.monitor.record(state)\n",
    "        state = process_frame(state)\n",
    "        reward += (info[\"score\"] - self.curr_score) / 40.            # 分数差奖励\n",
    "        self.curr_score = info[\"score\"]\n",
    "        if done:\n",
    "            if info[\"flag_get\"]:                  # 获得旗子\n",
    "                reward += 50\n",
    "            else:\n",
    "                reward -= 50                \n",
    "        if self.world == 7 and self.stage == 4:\n",
    "            if (506 <= info[\"x_pos\"] <= 832 and info[\"y_pos\"] > 127) or (\n",
    "                    832 < info[\"x_pos\"] <= 1064 and info[\"y_pos\"] < 80) or (\n",
    "                    1113 < info[\"x_pos\"] <= 1464 and info[\"y_pos\"] < 191) or (\n",
    "                    1579 < info[\"x_pos\"] <= 1943 and info[\"y_pos\"] < 191) or (\n",
    "                    1946 < info[\"x_pos\"] <= 1964 and info[\"y_pos\"] >= 191) or (\n",
    "                    1984 < info[\"x_pos\"] <= 2060 and (info[\"y_pos\"] >= 191 or info[\"y_pos\"] < 127)) or (\n",
    "                    2114 < info[\"x_pos\"] < 2440 and info[\"y_pos\"] < 191) or info[\"x_pos\"] < self.current_x - 500:\n",
    "                reward -= 50\n",
    "                done = True\n",
    "        if self.world == 4 and self.stage == 4:\n",
    "            if (info[\"x_pos\"] <= 1500 and info[\"y_pos\"] < 127) or (\n",
    "                    1588 <= info[\"x_pos\"] < 2380 and info[\"y_pos\"] >= 127):\n",
    "                reward = -50\n",
    "                done = True                                  # 针对这两个特定关卡\n",
    "\n",
    "        # if action in [2, 4]:  \n",
    "        #     reward += 0.5  # 跳跃小而正向的鼓励  针对simple_action\n",
    "\n",
    "        # # 卡住惩罚\n",
    "        # if info[\"x_pos\"] == self.current_x:\n",
    "        #     reward -= 0.3\n",
    "\n",
    "        self.current_x = info[\"x_pos\"]\n",
    "\n",
    "        return state, reward / 10., done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_score = 0\n",
    "        self.current_x = 40\n",
    "        return process_frame(self.env.reset())\n",
    "    \n",
    "def process_frame(frame):\n",
    "    if frame is not None:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)                           # 灰度处理\n",
    "        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.                    # 变尺寸\n",
    "        return frame\n",
    "    else:\n",
    "        return np.zeros((1, 84, 84))\n",
    "class CustomSkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip=4):  # 5.24 将skip = 4改成2，原理上可以执行连跳，也可能是操作不够精细不能连跳，所以尝试一下\n",
    "        super(CustomSkipFrame, self).__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(skip, 84, 84))\n",
    "        self.skip = skip\n",
    "        self.states = np.zeros((skip, 84, 84), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        last_states = []\n",
    "        for i in range(self.skip):\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if i >= self.skip / 2:\n",
    "                last_states.append(state)\n",
    "            if done:\n",
    "                self.reset()\n",
    "                return self.states.astype(np.float32), total_reward, done, info\n",
    "        max_state = np.max(np.concatenate(last_states, 0), 0)\n",
    "        self.states[:-1] = self.states[1:]\n",
    "        self.states[-1] = max_state\n",
    "        return self.states.astype(np.float32), total_reward, done, info                        # 输出shape是(4,84,84)\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.states = np.concatenate([state for _ in range(self.skip)], 0)\n",
    "        return self.states.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBaseNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ConvBaseNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear = nn.Linear(32 * 6 * 6, 512)  \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, nn.init.calculate_gain('relu'))\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward_conv(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.linear(x.view(x.size(0), -1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyNet(ConvBaseNet):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PolicyNet, self).__init__(num_inputs)           # num_inputs是父类需要的参数\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_conv(x)\n",
    "        return F.softmax(self.actor_linear(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(ConvBaseNet):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueNet, self).__init__(num_inputs)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_conv(x)\n",
    "        return self.critic_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(gamma, lmbda, td_delta):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0.0\n",
    "    for delta in td_delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    return torch.tensor(advantage_list, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    ''' PPO算法,采用截断方式 '''\n",
    "    def __init__(self, state_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs  # 一条序列的数据用来训练轮数\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
    "                                                                       dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda,\n",
    "                                               td_delta.cpu()).to(self.device)\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,\n",
    "                                                            actions)).detach()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps,\n",
    "                                1 + self.eps) * advantage  # 截断\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "            critic_loss = torch.mean(\n",
    "                F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, window_size):\n",
    "    cumulative_sum = np.cumsum(np.insert(a, 0, 0)) \n",
    "    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n",
    "    r = np.arange(1, window_size-1, 2)\n",
    "    begin = np.cumsum(a[:window_size-1])[::2] / r\n",
    "    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n",
    "    return np.concatenate((begin, middle, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d265bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_policy_agent(env, agent, num_episodes, save_dir, num_local_steps, resume=False,\n",
    "                          actor_model_path=None, critic_model_path=None):\n",
    "    return_list = []\n",
    "    saved_models_info = []  # 用于记录每次保存模型时的 (episode_num, return)\n",
    "    start_episode = 0\n",
    "\n",
    "    # 加载指定模型\n",
    "    if resume:\n",
    "        if actor_model_path and critic_model_path:\n",
    "            agent.actor.load_state_dict(torch.load(actor_model_path))\n",
    "            agent.critic.load_state_dict(torch.load(critic_model_path))\n",
    "            print(f\"Resumed training from provided model files:\\n- Actor: {actor_model_path}\\n- Critic: {critic_model_path}\")\n",
    "\n",
    "            # 尝试从路径中提取 episode 编号（可选优化）\n",
    "            try:\n",
    "                start_episode = int(os.path.basename(actor_model_path).split('_')[-1].split('.')[0])\n",
    "            except Exception as e:\n",
    "                print(\"Warning: Failed to parse episode number from filename. Start_episode set to 0.\")\n",
    "                start_episode = 0\n",
    "        else:\n",
    "            raise ValueError(\"To resume training, both actor_model_path and critic_model_path must be provided.\")\n",
    "\n",
    "    for i in range(start_episode // (num_episodes // 10), 10):\n",
    "        with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):\n",
    "                current_episode_num = int(num_episodes / 10) * i + i_episode + 1\n",
    "                if current_episode_num <= start_episode:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                episode_return = 0\n",
    "                transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                step_count = 0\n",
    "\n",
    "                while not done and step_count < num_local_steps:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done, _ = env.step(action) \n",
    "                    transition_dict['states'].append(state)\n",
    "                    transition_dict['actions'].append(action)\n",
    "                    transition_dict['next_states'].append(next_state)\n",
    "                    transition_dict['rewards'].append(reward)\n",
    "                    transition_dict['dones'].append(done)\n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                    step_count = 0\n",
    "\n",
    "                return_list.append(episode_return)\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "                if (i_episode+1) % 10 == 0:\n",
    "                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "                    if (current_episode_num) >= 250:\n",
    "                        torch.save(agent.actor.state_dict(), f'{save_dir}/ppo_mario_actor_{current_episode_num}.pth')  \n",
    "                        torch.save(agent.critic.state_dict(), f'{save_dir}/ppo_mario_critic_{current_episode_num}.pth')\n",
    "                        saved_models_info.append((current_episode_num, episode_return))  # 保存模型信息\n",
    "                pbar.update(1)\n",
    "\n",
    "    # 输出 return 最好的模型保存点\n",
    "    if saved_models_info:\n",
    "        best_model = max(saved_models_info, key=lambda x: x[1])\n",
    "        print(f\"Best model saved at episode {best_model[0]} with return {best_model[1]:.3f}\")\n",
    "    else:\n",
    "        print(\"No models were saved.\")\n",
    "\n",
    "    return return_list                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757011a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO_para\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2  \n",
    "num_episodes = 2000\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "epochs = 10\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# env_para\n",
    "world = 1\n",
    "stage = 1\n",
    "action_type = \"simple\"\n",
    "\n",
    "# train_on_policy\n",
    "save_dir = './model'  # 模型保存目录\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_local_steps = 512\n",
    "resume = True  # False\n",
    "actor_model_path = './model/ppo_mario_actor_250.pth'  # None\n",
    "critic_model_path = './model/ppo_mario_critic_250.pth'  # None\n",
    "\n",
    "env= create_train_env(world, stage, action_type)  \n",
    "env.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = PPO(state_dim, action_dim, actor_lr, critic_lr, lmbda,\n",
    "            epochs, eps, gamma, device)\n",
    "\n",
    "return_list = train_on_policy_agent(env, agent, num_episodes, save_dir, num_local_steps, resume, actor_model_path, critic_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('PPO on {}'.format(\"SuperMarioBros-{}-{}-v3\".format(world, stage)))\n",
    "plt.show()\n",
    "\n",
    "mv_return = moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('PPO on {}'.format(\"SuperMarioBros-{}-{}-v3\".format(world, stage)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bfdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_actor_dir, test_world, test_stage, test_num_steps):\n",
    "    env = create_train_env(test_world, test_stage, action_type=\"simple\")  \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    actor = PolicyNet(state_dim, action_dim).to(device)\n",
    "    actor.load_state_dict(torch.load(test_actor_dir, map_location=device))\n",
    "    actor.eval()\n",
    "\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while 1:\n",
    "        env.render()\n",
    "        time.sleep(0.02)\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 4, 84, 84)\n",
    "        with torch.no_grad():\n",
    "            probs = actor(state_tensor)\n",
    "        action = torch.argmax(probs).item()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        if done or step_count >= test_num_steps:\n",
    "            state = env.reset()\n",
    "            step_count = 0\n",
    "\n",
    "    env.close()\n",
    "    print(f'Total reward: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147edeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor_dir = './model/ppo_mario_actor_470.pth'\n",
    "test_world = 1\n",
    "test_stage = 1\n",
    "test_num_steps = 200\n",
    "\n",
    "test_model(test_actor_dir, test_world, test_stage, test_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb70cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchnote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
